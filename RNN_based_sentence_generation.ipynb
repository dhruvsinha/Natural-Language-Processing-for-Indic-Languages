{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_based_sentence_generation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Paragraph generation using Transformer models:\n",
        "\n",
        "We will attempt to generate a paragraph of text in Hindi language based on a given prompt. We will train the model on the texts taken from wikipdia artices in Hindi language. \n"
      ],
      "metadata": {
        "id": "h267_xzWgoQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.11.2"
      ],
      "metadata": {
        "id": "pfV4WmEc5hx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On Colab, you'll see ('1.10.2+cu102', '0.11.2')\n",
        "import torch, torchtext\n",
        "torch.__version__, torchtext.__version__"
      ],
      "metadata": {
        "id": "VmXOQMwr5tLY",
        "outputId": "2c3cce52-7d6f-4a13-b2d1-cc2be80d1150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.10.2+cu102', '0.11.2')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import Vectors\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    DEVICE = torch.device('cuda')\n",
        "    print(\"Using cuda.\")\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "    print(\"Using cpu.\")\n",
        "\n",
        "random.seed(30255)\n",
        "np.random.seed(30255)\n",
        "torch.manual_seed(30255)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(30255)\n",
        "\n",
        "# Change the following to false when training on\n",
        "# the full set\n",
        "DEVELOPING = False    \n",
        "#DEVELOPING = True\n",
        "\n",
        "if DEVELOPING:\n",
        "    print('Small development version')\n",
        "    BATCH_SIZE = 4\n",
        "    EMBEDDING_SIZE = 20\n",
        "    MAX_VOCAB_SIZE = 5000\n",
        "    TRAIN_DATA_SET = \"lm-train-small.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev-small.txt\"\n",
        "    TEST_DATA_SET = \"lm-test-small.txt\"\n",
        "    BPTT_LENGTH = 8\n",
        "else:\n",
        "    print('Full version')\n",
        "    BATCH_SIZE = 32\n",
        "    EMBEDDING_SIZE = 300\n",
        "    MAX_VOCAB_SIZE = 100000\n",
        "    TRAIN_DATA_SET = \"lm-train.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev.txt\"\n",
        "    TEST_DATA_SET = \"lm-test.txt\"\n",
        "    BPTT_LENGTH = 32\n",
        "\n"
      ],
      "metadata": {
        "id": "YbwLnZ1S6N7D",
        "outputId": "7a27e334-0bb0-4059-cb14-f3c5b2bfd995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n",
            "Full version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For uploading data to Colab see, e.g., \n",
        "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \n",
        "# COLAB = False\n",
        "COLAB = True\n",
        "if COLAB:\n",
        "    from google.colab import drive \n",
        "    drive.mount('/content/gdrive')\n",
        "    PATH = \"gdrive/My Drive/AML_Project/datasets_hindi_wiki\"\n",
        "else:\n",
        "    PATH = \"/Users/tiru/Documents/Documents_new/Adv_ML/hindi_wiki_55k\"\n",
        "    \n",
        "LOG_FILE = \"language-model.log\""
      ],
      "metadata": {
        "id": "9_4nob7dGoS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8cc77e-2679-4cfc-e2d0-d6446e5934cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This command is to unmount gdrive - need it sometimes\n",
        "#!fusermount -u gdrive"
      ],
      "metadata": {
        "id": "AX6BNsPo6yb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = torchtext.legacy.data.Field(lower=True)\n",
        "\n",
        "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=PATH, \n",
        "    train=TRAIN_DATA_SET, validation=DEV_DATA_SET, test=TEST_DATA_SET, text_field=TEXT)\n",
        "\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "\n",
        "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
        "\n",
        "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, device=DEVICE, bptt_len=BPTT_LENGTH, \n",
        "    repeat=False)"
      ],
      "metadata": {
        "id": "QL2LOrSR6z7Q",
        "outputId": "7b89d2b5-0f34-4b9c-ec67-4dcaeca77094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 100002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(train_iter)\n",
        "batch=next(it)\n",
        "print(\"The first three text/target sequences from the first batch are:\\n\")\n",
        "indent = \" \" * 4\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()\n",
        " \n",
        "print(f\"Each sequence has BPTT_LENGTH = {BPTT_LENGTH}.\\n\")\n",
        "print(\"Also the sequences continue in the next batch!\\n\")\n",
        "batch = next(it)\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "m85m3IXt7OGP",
        "outputId": "5dda0de3-ff26-4100-8d30-a536ddc351c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first three text/target sequences from the first batch are:\n",
            "\n",
            "     Text Sequence 0: <eos> भारतीय मोर या नीला मोर दक्षिण एशिया के देशी तीतर परिवार का एक बड़ा और चमकीले रंग का पक्षी है, दुनिया के अन्य भागों में यह अर्द्ध-जंगली के रूप में परिचित\n",
            "     Target Sequence 0: भारतीय मोर या नीला मोर दक्षिण एशिया के देशी तीतर परिवार का एक बड़ा और चमकीले रंग का पक्षी है, दुनिया के अन्य भागों में यह अर्द्ध-जंगली के रूप में परिचित है।\n",
            "\n",
            "     Text Sequence 1: पतला स्तर होता है, परंतु स्त्रियों में <unk> गुहा, गर्भाशय गुहा तथा <unk> द्वारा यह बाह्य वातावरण में खुलती है। इस पेरिटोनियम कला की परतों के द्वारा आशय उदर गुहा में लटके\n",
            "     Target Sequence 1: स्तर होता है, परंतु स्त्रियों में <unk> गुहा, गर्भाशय गुहा तथा <unk> द्वारा यह बाह्य वातावरण में खुलती है। इस पेरिटोनियम कला की परतों के द्वारा आशय उदर गुहा में लटके रहते\n",
            "\n",
            "     Text Sequence 2: चार वर्ष के उत्तर-स्नातक पर्यवेक्षित चिकित्सा-कार्य से लेकर तीन से छः वर्षों के डॉक्टर की उपाधि तक होती है, जिसमें नैदानिक स्थापन शामिल होती है। अमेरिका में नैदानिक मनोवैज्ञानिक के लगभग आधे\n",
            "     Target Sequence 2: वर्ष के उत्तर-स्नातक पर्यवेक्षित चिकित्सा-कार्य से लेकर तीन से छः वर्षों के डॉक्टर की उपाधि तक होती है, जिसमें नैदानिक स्थापन शामिल होती है। अमेरिका में नैदानिक मनोवैज्ञानिक के लगभग आधे छात्र\n",
            "\n",
            "Each sequence has BPTT_LENGTH = 32.\n",
            "\n",
            "Also the sequences continue in the next batch!\n",
            "\n",
            "     Text Sequence 0: है। नर, मोर, मुख्य रूप से नीले रंग के होते हैं साथ ही इनके पंख पर चपटे चम्मच की तरह नीले रंग की आकृति जिस पर रंगीन आंखों की तरह चित्ती बनी\n",
            "     Target Sequence 0: नर, मोर, मुख्य रूप से नीले रंग के होते हैं साथ ही इनके पंख पर चपटे चम्मच की तरह नीले रंग की आकृति जिस पर रंगीन आंखों की तरह चित्ती बनी होती\n",
            "\n",
            "     Text Sequence 1: रहते हैं। <eos> इन तंत्रों का वर्णन निम्नलिखित है : <eos> <unk> <unk> <unk> तथा इनकी रुधिर वाहिनियाँ आदि इस तंत्र के अंतर्गत हैं। वृक्क के दो गोले कटि कशेरुक के दोनों\n",
            "     Target Sequence 1: हैं। <eos> इन तंत्रों का वर्णन निम्नलिखित है : <eos> <unk> <unk> <unk> तथा इनकी रुधिर वाहिनियाँ आदि इस तंत्र के अंतर्गत हैं। वृक्क के दो गोले कटि कशेरुक के दोनों ओर\n",
            "\n",
            "     Text Sequence 2: छात्र पीएचडी कार्यक्रमों में प्रशिक्षित किए जा रहे हैं- यह एक ऐसा प्रारूप है, जिसमें अनुसंधान पर बल डाला जाता है; अन्य आधे छात्र <unk> कार्यक्रम के होते हैं, जो चिकित्सा-कार्य पर\n",
            "     Target Sequence 2: पीएचडी कार्यक्रमों में प्रशिक्षित किए जा रहे हैं- यह एक ऐसा प्रारूप है, जिसमें अनुसंधान पर बल डाला जाता है; अन्य आधे छात्र <unk> कार्यक्रम के होते हैं, जो चिकित्सा-कार्य पर केंद्रित\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
        "                 dropout=0.5):\n",
        "        ''' Initialize model parameters corresponding to ---\n",
        "            - embedding layer\n",
        "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \n",
        "              optionally more than one layer\n",
        "            - linear layer to map from hidden vector to the vocabulary\n",
        "            - optionally, dropout layers.  Dropout layers can be placed after \n",
        "              the embedding layer or/and after the RNN layer. Dropout within\n",
        "              an RNN is only applied when there are two or more num_layers.\n",
        "            - optionally, initialize the model parameters.\n",
        "            \n",
        "            The arguments are:\n",
        "            \n",
        "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\n",
        "            vocab_size: size of vocabulary\n",
        "            embedding_dim: size of an embedding vector\n",
        "            hidden_dim: size of hidden/state vector in RNN\n",
        "            num_layers: number of layers in RNN\n",
        "            dropout: dropout probability.\n",
        "            \n",
        "        '''\n",
        "        super(RNNLM, self).__init__()\n",
        "        \n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "        # Embedding layer definition: \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # RNN definition - depnding on input type requested for RNN\n",
        "        if rnn_type == 'LSTM':\n",
        "          self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "          self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
        "        elif rnn_type == 'RNN_TANH':\n",
        "          self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, nonlinearity='tanh', dropout=dropout)\n",
        "        elif rnn_type == 'RNN_RELU':\n",
        "          self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, nonlinearity='relu', dropout=dropout)\n",
        "        else:\n",
        "          print(\"Enter one of the four inputs : LSTM/GRU/RNN_TANH/RNN_RELU\")  \n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size) \n",
        "\n",
        "\n",
        "    def forward(self, input, hidden0):\n",
        "        ''' \n",
        "        Run forward propagation for a given minibatch of inputs using\n",
        "        hidden0 as the initial hidden state.\n",
        "\n",
        "        In LSTMs hidden0 = (h_0, c_0). \n",
        "\n",
        "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
        "        Return this as well so that it can be used to initialize the next\n",
        "        batch.\n",
        "        \n",
        "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
        "        the more efficient CrossEntropyLoss.  See \n",
        "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
        "        '''\n",
        "\n",
        "        embeds = self.embedding(input)\n",
        "        output_n, hidden_n = self.rnn(embeds, hidden0)\n",
        "        output_n = self.fc(output_n)\n",
        "\n",
        "        return output_n, hidden_n"
      ],
      "metadata": {
        "id": "pZnt4_BG7OIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data):\n",
        "    '''\n",
        "    Evaluate the model on the given data.\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "    it = iter(data)\n",
        "    total_count = 0. # Number of target words seen\n",
        "    total_loss = 0. # Loss over all target words\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "        hidden = None \n",
        "        for i, batch in enumerate(it):\n",
        "            ''' Do the following:\n",
        "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
        "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "                  the current batch. \n",
        "                - Call forward propagation to get output and final hidden state vector.\n",
        "                - Compute the cross entropy loss\n",
        "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
        "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
        "                  total count (of target words) and total loss see so far over all batches.\n",
        "            '''\n",
        "            text, target = batch.text, batch.target\n",
        "            if USE_CUDA:\n",
        "                text, target = text.cuda(), target.cuda()\n",
        "            output, hidden = model(text, hidden)\n",
        "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
        "                  \n",
        "            total_count += np.multiply(*text.size())\n",
        "            total_loss += loss.item()*np.multiply(*text.size())\n",
        "                \n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "4R5Km6mq7OK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
        "GRAD_CLIP = 1.\n",
        "NUM_EPOCHS = 12\n",
        "CUDA_LAUNCH_BLOCKING = \"1\"\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if h is None:\n",
        "        return None\n",
        "    elif isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "''' Do the following:\n",
        "    - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "      the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "      https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda -DONE\n",
        "    - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "      the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
        "      the provided repackage_hidden(). See\n",
        "      https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
        "    - Zero out the model gradients to reset backpropagation for current batch\n",
        "    - Call forward propagation to get output and final hidden state vector.\n",
        "    - Compute the cross entropy loss\n",
        "    - Run back propagation to set the gradients for each model parameter.\n",
        "    - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
        "      https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
        "    - Run a step of gradient descent. \n",
        "    - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
        "    - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
        "      your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
        "      copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
        "      https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
        "      in Sec 2.3.1 of Lecture notes by Cho: \n",
        "      https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
        "'''\n",
        "\n",
        "\n",
        "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, DROPOUT)\n",
        "print(\"model:\", model)\n",
        "print(\"use_cuda_status: \", USE_CUDA)\n",
        "if USE_CUDA:\n",
        "  model = model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "val_losses = []\n",
        "epoch_val_loss=[]\n",
        "best_model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, DROPOUT)\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Training Epoch #\", epoch+1)\n",
        "    model.train()\n",
        "    log_interval = 1000\n",
        "    it = iter(train_iter)\n",
        "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "    hidden = None\n",
        "\n",
        "    for i, batch in enumerate(it):\n",
        "      model.zero_grad()\n",
        "      text, target = batch.text, batch.target\n",
        "      if USE_CUDA:\n",
        "        text, target = text.cuda(), target.cuda()\n",
        "      output_n, hidden_n = model(text, hidden)\n",
        "      hidden = repackage_hidden(hidden_n)\n",
        "      loss = loss_fn(output_n.view(-1, output_n.size(-1)), target.view(-1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP, norm_type=2.0, error_if_nonfinite=False)\n",
        "\n",
        "      if i % log_interval == 0 and i > 0:\n",
        "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
        "\n",
        "      if i % 2000 == 0 and i > 0:\n",
        "        val_loss = evaluate(model, val_iter)\n",
        "        print(f'Iteration {i}, the validation loss is {val_loss:.3f}.')\n",
        "        val_losses.append(val_loss)\n",
        "        if val_loss <= min(val_losses):\n",
        "          torch.save(model.state_dict(), \"gdrive/My Drive/AML_Project/best_model.pt\")\n",
        "\n",
        "    epoch_loss = evaluate(model, val_iter)\n",
        "    epoch_val_loss.append(epoch_loss)\n",
        "\n",
        "best_model.load_state_dict(torch.load(\"gdrive/My Drive/AML_Project/best_model.pt\")) \n",
        "plt.plot(range(1, NUM_EPOCHS+1), epoch_val_loss)\n",
        "plt.title(\"Vocab Effect, LR=0.001, Dropout=0.5, Grad_clip=1, layers=2, embedding size = 300\")\n",
        "plt.savefig(\"Layer_effect.png\")"
      ],
      "metadata": {
        "id": "b5EIvXYS7ONq",
        "outputId": "d209c319-5c21-47c3-8f6a-7220a3f9c788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: RNNLM(\n",
            "  (embedding): Embedding(100002, 300)\n",
            "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=300, out_features=100002, bias=True)\n",
            ")\n",
            "use_cuda_status:  True\n",
            "Training Epoch # 1\n",
            "At iteration 1000 the loss is 7.117.\n",
            "At iteration 2000 the loss is 6.626.\n",
            "Iteration 2000, the validation loss is 6.519.\n",
            "At iteration 3000 the loss is 6.451.\n",
            "At iteration 4000 the loss is 6.129.\n",
            "Iteration 4000, the validation loss is 6.138.\n",
            "At iteration 5000 the loss is 6.189.\n",
            "Training Epoch # 2\n",
            "At iteration 1000 the loss is 5.842.\n",
            "At iteration 2000 the loss is 5.728.\n",
            "Iteration 2000, the validation loss is 5.890.\n",
            "At iteration 3000 the loss is 5.663.\n",
            "At iteration 4000 the loss is 5.304.\n",
            "Iteration 4000, the validation loss is 5.764.\n",
            "At iteration 5000 the loss is 5.542.\n",
            "Training Epoch # 3\n",
            "At iteration 1000 the loss is 5.198.\n",
            "At iteration 2000 the loss is 5.222.\n",
            "Iteration 2000, the validation loss is 5.688.\n",
            "At iteration 3000 the loss is 5.280.\n",
            "At iteration 4000 the loss is 4.848.\n",
            "Iteration 4000, the validation loss is 5.658.\n",
            "At iteration 5000 the loss is 5.184.\n",
            "Training Epoch # 4\n",
            "At iteration 1000 the loss is 4.824.\n",
            "At iteration 2000 the loss is 4.910.\n",
            "Iteration 2000, the validation loss is 5.652.\n",
            "At iteration 3000 the loss is 5.003.\n",
            "At iteration 4000 the loss is 4.515.\n",
            "Iteration 4000, the validation loss is 5.630.\n",
            "At iteration 5000 the loss is 4.975.\n",
            "Training Epoch # 5\n",
            "At iteration 1000 the loss is 4.586.\n",
            "At iteration 2000 the loss is 4.726.\n",
            "Iteration 2000, the validation loss is 5.668.\n",
            "At iteration 3000 the loss is 4.857.\n",
            "At iteration 4000 the loss is 4.375.\n",
            "Iteration 4000, the validation loss is 5.622.\n",
            "At iteration 5000 the loss is 4.804.\n",
            "Training Epoch # 6\n",
            "At iteration 1000 the loss is 4.459.\n",
            "At iteration 2000 the loss is 4.576.\n",
            "Iteration 2000, the validation loss is 5.678.\n",
            "At iteration 3000 the loss is 4.728.\n",
            "At iteration 4000 the loss is 4.216.\n",
            "Iteration 4000, the validation loss is 5.621.\n",
            "At iteration 5000 the loss is 4.705.\n",
            "Training Epoch # 7\n",
            "At iteration 1000 the loss is 4.308.\n",
            "At iteration 2000 the loss is 4.452.\n",
            "Iteration 2000, the validation loss is 5.680.\n",
            "At iteration 3000 the loss is 4.586.\n",
            "At iteration 4000 the loss is 4.137.\n",
            "Iteration 4000, the validation loss is 5.608.\n",
            "At iteration 5000 the loss is 4.575.\n",
            "Training Epoch # 8\n",
            "At iteration 1000 the loss is 4.239.\n",
            "At iteration 2000 the loss is 4.363.\n",
            "Iteration 2000, the validation loss is 5.679.\n",
            "At iteration 3000 the loss is 4.454.\n",
            "At iteration 4000 the loss is 3.999.\n",
            "Iteration 4000, the validation loss is 5.612.\n",
            "At iteration 5000 the loss is 4.508.\n",
            "Training Epoch # 9\n",
            "At iteration 1000 the loss is 4.156.\n",
            "At iteration 2000 the loss is 4.270.\n",
            "Iteration 2000, the validation loss is 5.674.\n",
            "At iteration 3000 the loss is 4.431.\n",
            "At iteration 4000 the loss is 3.942.\n",
            "Iteration 4000, the validation loss is 5.603.\n",
            "At iteration 5000 the loss is 4.431.\n",
            "Training Epoch # 10\n",
            "At iteration 1000 the loss is 4.041.\n",
            "At iteration 2000 the loss is 4.221.\n",
            "Iteration 2000, the validation loss is 5.687.\n",
            "At iteration 3000 the loss is 4.397.\n",
            "At iteration 4000 the loss is 3.883.\n",
            "Iteration 4000, the validation loss is 5.612.\n",
            "At iteration 5000 the loss is 4.425.\n",
            "Training Epoch # 11\n",
            "At iteration 1000 the loss is 3.989.\n",
            "At iteration 2000 the loss is 4.131.\n",
            "Iteration 2000, the validation loss is 5.709.\n",
            "At iteration 3000 the loss is 4.323.\n",
            "At iteration 4000 the loss is 3.782.\n",
            "Iteration 4000, the validation loss is 5.627.\n",
            "At iteration 5000 the loss is 4.383.\n",
            "Training Epoch # 12\n",
            "At iteration 1000 the loss is 3.913.\n",
            "At iteration 2000 the loss is 4.074.\n",
            "Iteration 2000, the validation loss is 5.723.\n",
            "At iteration 3000 the loss is 4.272.\n",
            "At iteration 4000 the loss is 3.742.\n",
            "Iteration 4000, the validation loss is 5.639.\n",
            "At iteration 5000 the loss is 4.315.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAEICAYAAACtaWlhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU933v/9dntCIhkDBCYscbNsgG7GBwEseOjS05XhI3v6aNQxonvYnrXmfp3iZNb52kTdPeLslt2uv4pmmSBid1kzgbqcHBcdykMTZeAAPGxpgdhAAJEEhomc/vj+8ZGA1aRuvRSO/n4zGPOXO2+cycmTPv+c73nDF3R0REREREzknEXYCIiIiIyGijkCwiIiIikkEhWUREREQkg0KyiIiIiEgGhWQRERERkQwKySIiIiIiGRSSZVDM7K1mtm8I1/fbZlZvZs1mdoGZvdnMXo1u3zVU9yMy1g32vWlm7zezn6fdbjazi4amusEzsyfN7INx15HLzOyrZvYXQ7SuLq+Xbqaf3V5mttLM1g7F/WbLzD5hZl8eyfuU3KeQPIaY2WNm9uluxr/DzA6ZWX4cdWXU8qSZtUYfuKnLD6NpBcDfA7XuPtHdjwKfBr4Y3f7eIO53l5nd3I/5HzCzb/Syrpao9kPRB83EAdRkZvbXZnY0uvy1mVkv87/HzHab2Skz+56ZTUmbNsXMHo2m7Taz96RNm25mPzCzA2bmZjavHzXOi5ZJbat6M/uRmd3S38c7Evr6oO5j2ay3RxRAkxmv43v6cV9lZvb30WvplJntMbNvm9nygdQ+EqL34M6hXKeZfcbMNptZh5k9MJTrHq3MbJqZfTN6Px43s1+M5u0+HNx9lbvXjvB9ftbdR8WXKjP7GzPba2Ynov31JzKmLzGz58zsdHS9JG1avz43ZHAUkseWrwHv7eYN8xvAKnfviKGm7nw4+sBNXe6MxlcBxcCWtHnnZtweLe5094nAEuAq4OMDWMe9wF3AYmARcCfwW93NaGY1wJcI27IKOA38c9os/wS0RdNWAv83WgYgCTwG/H8DqDGlPHq8i4HHgUfN7P091Br7l7EBynp7RA5kvI6/ls2dmFkR8ARwJXAHMAlYAHwLeFsPy+Tqc9qXHcAfAavjLiQbQ7QdJgLPAm8AphD226sH8kVbcta/AJe7+yTgTcBKM3sngJkVAt8HvgFUEF4f34/GQ//3UzIY7q7LGLkAE4DjwPVp4yqAVsIbqgj4PHAgunweKEqb9x3Ai8AJ4DXg1mj8B4BtwElgJ/Bbacu8FdgHfAI4AuwCVvZS45PAB7sZPx84BTjQTAgRrxECXks0rgiYTNjBHAT2A38B5KWt50NptW4Frgb+LWM9f5TFc/kA8I0epu0Cbk67/TfA6gFsr/8G7k27/T+Ap3uY97PAw2m3LyaE4jKgNBqenzb934DPZawjP3p+5/WjxnnRMvkZ4/8AqAcSac/JHwObgDPRfb2d8AWnKdruCzKew49H26gR+FegOGM77gCOAT8AZvRUT+o1RQiarUBntJ2bhnF7vBXYN8D36Qej129pH/M5cD/wKvB6NO4LwF7Ce/Q54C0Z7/+vRs/nVuAPs6kRmA18F2gAjhJ+uQF4P/DzjHouiYa/CjxI+MJ0EvgZMHcgz0e0vm8AD/RzmSeJ9iXR++GJqP4jwCrCFzui5+E7Gcv+H+AL0XCP+5ToOfgF8A/Ruv8CuCR6vMej+/r3gT7utHpOAG/Ict5s620i7K/fFI3fCxwG7klbV6/bEbg8mnYM2A78Wtq0CwjvzRPAM8BnMl4vtwAvR8/TF6N1fzCtzszX1n2E13oT4Uu/RdPygL+LnuvXgQ/TzT4pbV1/HD0vJ6OaV0TjHyDap0f1NKddOlKvP2AG8B3C++F14KOD3b59bM+ZwGaizyWgNqrf0ubZw7nP46z3U7oM/qKW5DHE3VuAR4D3pY3+NeBld98I/ClwLaH1czGwDPgkgJktA75O+EApB64nBBkIO9ZUi9cHgH8ws6vT7qMamEp4s98DPGRml/Wz9leAVMtnubvf5O4XE3YOd3poqTtD2Kl3ED6oriLsUFL93N5F2BG+L6r17cBRd/+NjPX8TX9q642ZzSK0/u1IG/cnZtbU0yVt8RpgY9rtjWnPQaYu87r7a0TBOLp0RM9hNusaCt8FpgHp2/lu4HbC6+ci4JvA7wCVwI+BH6a1hkBo8a4jBJz5nHst3gT8FeG1Ox3YTWhl7ZW7byN80P4y2s7l0fqGY3sATIu6n7xuZv9gZqV91Ri5GVjj7qeymPcuYDmwMLr9LOH9OwV4GPgPMyuOpv054bm8mPC89tn9w8zygB8RnuN5hPdwn891ZCUhGE0lfLlelbbeTb085//c0woHwQivmRmEL0uzCfsCCAH8VjNLvR7ygXcT9nfQyz4lspwQNquAvyQ85rWEBohZwD+eLWIAjzv6Kb2QtH1IH7KpdxMhxD5M2J7XRPO/F/hiRqt1t9sxej0/Hq1jGuE5+2czS70W/4nwpXQ68JvRJfWYphL2EZ+M1vsa8OY+HtcdUZ2LCO/9umj8hwj72CWERo8ej02JPnc+DFzj7mXROnZlzufuZ3/NBK4jfLH8vpklgB8S3vszgRXA75hZXeY6ovvLdt/S07LNhEamUsLzDGGfs8mjBBzZxLl9UX/3UzIYcad0XYb2QnjDNxG1yhFaFX43Gn4NuC1t3jpgVzT8JeAfsryP7wEfi4bfSthhl6ZNfwT4sx6WfZLQVaAp7fKZaNo8zm8l3EXUakv4kDoDTEibfjfw02h4Taqubu737HqyfIwP0HtLcjOhpcKBdUStVv3cVp2En9xSty+N1mfdzLsOuC9j3P7o+X8LcChj2oeAJzPGDWVLcnE0/s1pz8lvpk3/M+CRtNuJVL1p89+XNv024LVo+F+Av0mbNhFoj2rp7jXyJD20UA3j9qgmBNcEcCHwFPClLO/nJ6S18hM+/JsILXLb08Y7cFMf62oEFkfDO4lam6Lb99JHSzLwRkKL2XmtcpnPJee3JH8rYxt1ArMH+NwPqiW5m2l3AS+k3f5P4EPR8B3A1mi4r33K+4E9Gev+OvAQMGsgjzVjXZMIrYgfz3L+bOp9NW3aldF2q0obdxRY0td2BH4d+K+M+/8S4ctYHuE9mf5++Wzq9UJoqHg6bZoRwmBvLcnXpd1+BPiTaPgJuv56eTM9tCQTvggcjuYpyJj2ABn7dMIX+F3Au6Pby7vZ3h8H/nWw27qH7WmELzqfAsqicX+Wvk2icas419Kd9X5Kl8Ff1JI8xrj7zwk/S91lZhcTWotT31BnEFqMUnZH4yDsFF/rbp1m9jYze9rMjkXfjm8jtA6kNHrXVrH09Xbno+5ennb5sywf3lygADiY9k39S4RWjl4fwzC4y0NLxVsJP0lO7X32bjUTPiRTJgHNHu35+pg3Nf/JPqYNl5nR9bG0cXvThru81tw9GU2f2cP86a+ZzGWbCR/s6csOh6y3h7sfcvet7p5099cJ/Wqz7fN9lND6llrXix5avd9J6FKULv05wsz+wMy2RQd8NRF+ek+99mZw/nPal9nAbh/Y8Qpn7yvaRsfo/X0/bMysysy+ZWb7zewEIXSnvye/RmhFJbr+t2i4r30KZGwDwrY24Bkz22Jmv8kAmNkEQqvl0+7+V1kulk299WnDLQDunjkuvSW5p+04F1ie0TK6kvAFsZLwpbun11uX12L0Hsp8HjMdShs+nVZj5uu6x/W4+w7Cr1cPAIej10S3r0kLB4p/m9CNLfXryVxgRsZj/gThy8mQ8+AFwjb5VDS6r/15fz43ZJAUksemrxO+yb+X8LNuagd5gLATSJkTjYOw47k4c0XRQUbfAf6W0BpRTvjpPP3gwIqMn5rT1zuU9hJaUaamBexJ7l6TNv28xxAZlh2Iu/+M0Brzt6lxFk411NzTJW3xLYRuLymL6fkgxS7zWjgVVxHwSnTJN7NLs1zXUPgVQovN9rRx6c9xl9dadDDpbEJrcsrstOH010zmsqWEn473E/qtA5SkLVvdQw2p5Ydje2Ryst+frgNqs+yecfbxmNlbCAHt14CK6L14nHPvxYOc/5z2ZS8wZ4AHpJ29r+jn+ylE2zAKjz095w8O4L768lnCc3Wlh4Oh3kvXfdT3gEVmdgWhJTnVNaSvfQpkvKaiL0gfcvcZhAOm/tnMLunP4472q98jtK7256CrbOrtr562417gZxkNGhPd/bcJvz500PPr7WDGei1j3v44SOjWcl693XH3h939OsI+xIG/7mHWfyT8evPJtHF7Cf3/0x9zmbvf1t0K+rFv6Us+5z67thBeq+mv30Wc2xcNZj8l/aSQPDZ9nfBz04cILSgp3wQ+aWaVUZ+x/0VocYHwE/cHzGyFmSXMbKaZXU7oK1dEtFM0s7cR+sBl+pSZFUYf5HcA/zHUD8rdDxL6Av6dmU2K6rzYzG6IZvky8Adm9gYLLjGzVNiqJ/STPcvC6bfe38tdJsysOO2S2cqX8nngFjNbHNX5We961oMul7Tlvg78XvRczwB+nxC4u7MKuNPM3hKFq08D33X3k1Er/neBT5tZqZm9mXAQZqq1DAv9VlP1F9m5fqyp09092cvzcFbUYvdhwk+uH49aiLvzCHB79HoqiB7bGcJBJyn3m9ksC6ey+1Pg36Px3yS8FpdEz/lngfXuvsvdGwhh+b1mlhe14qV/MaoHZlla3+fh2B5mdqOZzY1eZ7OBzxGOSE9N/6qZdbtsdD8HCWcIuSJ6HMXA0h7mTykjBJMGwpei/0XXFqVHgI+bWYWFvvIf6WN9EA64Ogh8LnrtFEevn2zcZmbXRc/1ZwgtonsB3L2ml+f8vtQKzKwgeuyJ6DEVW+gnnX76wXlZ1FJGaGE7bmYzCcdWnOXurUSthsAz7r4nGt/XPuU8Zvau6PmF0N3FCQcGZ/W47VwLZgvhILpkxvp7fNwDqTcLPW3HHwHzzew3ou1UYGbXmNkCd+8k7HMeMLMSC/2U70lb52qgxszeaeEL2Efp+mW2Px4BPha9L8sJB+Z1y8wuM7Obov1GK+E5Pm8fZWa/BdxAOMg8ffozwEkz+2MzmxC9N68ws2u6u79+7FvS7zthZr8VvU/NwvFA9xO+PEPoRtQJfNTMiqL9LYRuJ9C/zw0ZJIXkMcjddxHCSCnh6OOUvwA2EA4C2Aw8H43D3Z8hOiiP0DqVOsr5JGEH9wjhA+E9GeuE8DNZI6H1YRWhr+nLvZT4Rev6jfu5fjy89xGCe+qsCN8m+una3f+DcGDNw4Sfpr5HaBWBcFDPJy38hPYH0QfCBcDTvdzX3YSdbOrSbVeOKLh9nfCloz++RPi5dTPwEuGD5UupidFz85boPrYQDkpbRWjBLQP+Z9q6/ifh7AaHCSHzt6NlUlJn9oBwxHlL2rTZhL7rvWkys1NRrbcB73L3r/Q0s7tvJ7Tm/SOh+8+dhAMn29Jme5jwgb+T8NymXos/IfTL+w4hwF1MOGgo5UOEEHSUcMBKevB+gtCqcsjMjvTxmDJlvT0I/Qj/m9Cy/d/RMh9NW1ePz2kU2G4kvIZXE/VFJhy09Gu91LeGcCq/Vwg/bbfS9afnT0XjXyc8r/+WuYJuaukkbJtLCAe37iP0Rc3Gw4QvS8cIpzN7b++zd+v/EV6LdxO+KLUQTnMIUVcQuv760JNPEQ7qOk54Tr/bzTxfI/TRzXxeetyn9OAaYL2FlsIfEI6D6M/5o99EaEioJbyvmjNeW3097v7W25dut2O0768lvPcOEPbzf825L9sfJnSJOEQIaf+aWqG7HwHeRfjyeJTQb7avfUxP/h/h9bwJeIHwS2YHIUhmKoru80hU1zS6Pz3n3YRGkwNpz/8novfDHYTjBF6P1vNlQremofQrhH3eSUJD1T9GF6J95F2E7dxEOCDyrrR9Z6/7KRlaqVOsiIwrZnYdcL+73x13LXEzsxcJp0k6OoL3uYtwEM9PRuo+R0r0BWwjsMjd2+OuZzhEreT73P2Tfc07iPv4JNDg7kMSAMxsDuELYrW7nxiKdQ6HoX7cY42FXzMfdPe5fc4sMkhj9QT1Ir3ycIDjgP6Zbaxx9yV9zyXZilp8FsRdR65z9yH5u2QIP3EDv0c4a8CoDcgwtI97LLBwgOONhNbkKkKr96OxFiXjRlbdLcys3MJfpr5s4cjqN2ZMNzP7P2a2w8J5Iq9Om3aPmb0aXe45f+0iIjJczGyO9XxwUTYH9+U0C334TxD+3OLPYy5H+s8I3WkaCd0tttH/rm0iA5JVdwsz+xrhfIlfjn5KLHH3prTptxEOErmNcJ7BL7j7cgsH5GwgHJDihH+IeoO7Nw79QxERERERGRp9tiSb2WTCv6/9C4SfEtMDcuQdwNc9eBooN7PphD+reNzdj0XB+HHg1iF9BCIiIiIiQyybPskXEk459K8WTnH1HOFo3vQ/j5hJ16Os90Xjehrfq6lTp/q8efOyKE1EREREZGCee+65I+5e2d20bEJyPuHUOh9x9/Vm9gXgTwinaBoyZnYv4W9UmTNnDhs2bBjK1YuIiIiIdGFmPf47aTYH7u0jnOpnfXT724TQnG4/Xf8FZ1Y0rqfx53H3h9x9qbsvrazsNtCLiIiIiIyIPkOyux8C9prZZdGoFYSTmKf7AfC+6CwX1wLHo38GWkP4+9UKM6sgnJh8zdCVLyIiIiIy9LI9T/JHgFXRmS12Ev4y9j4Ad3+Q8A84twE7gNOEf27D3Y+Z2WeAZ6P1fNrdjw1h/SIiIiIiQ25U/uPe0qVLXX2SRURERGQ4mdlz7r60u2lZ/ZmIiIiIiMh4opAsIiIiIpJBIVlEREREJINCcuTRF/bxrWf2xF2GiIiIiIwCCsmRH208yD8+sYPReCCjiIiIiIwsheRIXU01+5ta2HLgRNyliIiIiEjMFJIjKxZMI2GwZsuhuEsRERERkZgpJEcumFjENfOmKCSLiIiIiEJyurqaal6pb+b1I6fiLkVEREREYqSQnKa2pgqAtWpNFhERERnXFJLTzKoo4YqZk9TlQkRERGScU0jOULewmuf3NHH4RGvcpYiIiIhITBSSM9TWVAOwdmt9zJWIiIiISFwUkjPMr5rIvAtK1OVCREREZBxTSM5gZtTVVPPL145yvKU97nJEREREJAYKyd2orammI+n89OXDcZciIiIiIjFQSO7GVbPLmVZWxNqt6nIhIiIiMh4pJHcjkTBuWVjFk9sbaG3vjLscERERERlhCsk9qKup5nRbJz9/9UjcpYiIiIjICFNI7sG1F11AWXG+znIhIiIiMg4pJPegMD/BTZdP4yfb6unoTMZdjoiIiIiMIIXkXtTVVNN4up1ndzXGXYqIiIiIjCCF5F7cML+SwvyEulyIiIiIjDMKyb0oLcrn+kun8vjWetw97nJEREREZIQoJPehtqaa/U0tbDlwIu5SRERERGSE5Gczk5ntAk4CnUCHuy/NmF4BfAW4GGgFftPdX8pm2dHu5gVVJAzWbDnEFTMnx12OiIiIiIyA/rQk3+juS3oIuZ8AXnT3RcD7gC/0Y9lRbUppIcsunKJ+ySIiIiLjyFB1t1gIPAHg7i8D88ysaojWHbu6mmpeqW/m9SOn4i5FREREREZAtiHZgbVm9pyZ3dvN9I3AOwHMbBkwF5iV5bJEy91rZhvMbENDQ0P2j2AE3LIw5H21JouIiIiMD9mG5Ovc/WrgbcD9ZnZ9xvTPAeVm9iLwEeAFQh/kbJYFwN0fcvel7r60srKy3w9kOM2qKOGKmZMUkkVERETGiaxCsrvvj64PA48CyzKmn3D3D7j7EkKf5EpgZzbL5oq6hdW8sKeJ+hOtcZciIiIiIsOsz5BsZqVmVpYaBmqBlzLmKTezwujmB4Gn3P1ENsvmirorqgFYu7U+5kpEREREZLhl05JcBfzczDYCzwCr3f0xM7vPzO6L5lkAvGRm2wndKj7W27JD+xBGxqXTJnLh1FLWqsuFiIiIyJjX53mS3X0nsLib8Q+mDf8SmJ/tsrnIzKitqeJf/ut1jre0M3lCQdwliYiIiMgw0T/u9UNdTTUdSeenLx+OuxQRERERGUYKyf2wZFY508qKdJYLERERkTFOIbkfEgnjloVVPLm9gdb2zr4XEBEREZGcpJDcT3U11bS0d/Jfrx6JuxQRERERGSYKyf107UUXUFacry4XIiIiImOYQnI/FeYnWHH5NNZtq6ejMxl3OSIiIiIyDBSSB6CupprG0+08u6sx7lJEREREZBgoJA/ADZdVUpSfUJcLERERkTFKIXkASgrzecullTy+tR53j7scERERERliCskDVFtTxf6mFl7afyLuUkRERERkiCkkD9DNC6pIGOpyISIiIjIGKSQP0JTSQpZdOEUhWURERGQMUkgehLqaal493MzOhua4SxERERGRIaSQPAi1NdUArNlSH3MlIiIiIjKUFJIHYWb5BK6cOZm1W9XlQkRERGQsUUgepLqaKl7Y00T9ida4SxERERGRIaKQPEh1UZeLtVvV5UJERERkrFBIHqRLpk3kwqmlrNVZLkRERETGDIXkQTIzamuq+OVrRzl+uj3uckRERERkCCgkD4G6mmo6ks4T29XlQkRERGQsUEgeAktmlTOtrIg1Lykki4iIiIwFCslDIJEIXS5+9koDre2dcZcjIiIiIoOkkDxE6mqqaWnv5L9ePRJ3KSIiIiIySArJQ+Taiy5gUnE+a3SWCxEREZGcl1VINrNdZrbZzF40sw3dTK8ws0fNbJOZPWNmV6RNu9XMtpvZDjP7k6EsfjQpyEuwYkEV67bV09GZjLscERERERmE/rQk3+juS9x9aTfTPgG86O6LgPcBXwAwszzgn4C3AQuBu81s4SBrHrVqF1bReLqdZ3Ydi7sUERERERmEoepusRB4AsDdXwbmmVkVsAzY4e473b0N+BbwjiG6z1HnhssqKcpPsHaLznIhIiIiksuyDckOrDWz58zs3m6mbwTeCWBmy4C5wCxgJrA3bb590bjzmNm9ZrbBzDY0NDRkW/+oUlKYz1surWTtlkO4e9zliIiIiMgAZRuSr3P3qwndJu43s+szpn8OKDezF4GPAC8A/ToXmrs/5O5L3X1pZWVlfxYdVepqqjhwvJXN+4/HXYqIiIiIDFBWIdnd90fXh4FHCd0o0qefcPcPuPsSQp/kSmAnsB+YnTbrrGjcmHXzgiryEqazXIiIiIjksD5DspmVmllZahioBV7KmKfczAqjmx8EnnL3E8CzwKVmdmE0/d3AD4byAYw2FaWFLJs3Rf2SRURERHJYNi3JVcDPzWwj8Ayw2t0fM7P7zOy+aJ4FwEtmtp3QJeNjAO7eAXwYWANsAx5x9y1D/SBGm7qaKl493MzOhua4SxERERGRAcjvawZ33wks7mb8g2nDvwTm97D8j4EfD6LGnFNbU80DP9zKmi31/PZbJ8ZdjoiIiIj0k/5xbxjMKJ/AlTMnq1+yiIiISI5SSB4mdTVVvLi3iUPHW+MuRURERET6SSF5mNTVVAPw+Fa1JouIiIjkGoXkYXLJtIlcNLWUNTrLhYiIiEjOUUgeJmZGbU01T+88yvHT7XGXIyIiIiL9oJA8jOpqquhIOk9sV2uyiIiISC5RSB5Gi2eVUzWpiDUvKSSLiIiI5BKF5GGUSBi1C6v52SsNtLZ3xl2OiIiIiGRJIXmY1dZU0dLeyVOvNMRdioiIiIhkSSF5mF170QVMKs7XWS5EREREcohC8jAryEuwYkEV616up6MzGXc5IiIiIpIFheQRUFdTRdPpdp55/VjcpYiIiIhIFhSSR8D18yspyk+wZov+fU9EREQkFygkj4CSwnyun1/J2q31uHvc5YiIiIhIHxSSR0hdTTUHj7eyef/xuEsRERERkT4oJI+QFZdPIy9h6nIhIiIikgMUkkdIRWkhy+ZN0angRERERHKAQvIIqqupYsfhZl5raI67FBERERHphULyCKqtqQZQlwsRERGRUU4heQTNKJ/AolmT1eVCREREZJRTSB5hdTXVbNzbxKHjrXGXIiIiIiI9UEgeYXU1VQA8vlVdLkRERERGK4XkEXbJtDIuqixVlwsRERGRUUwhOQZ1NdU8vfMox0+3x12KiIiIiHRDITkGtQur6Eg6615Wa7KIiIjIaJRVSDazXWa22cxeNLMN3UyfbGY/NLONZrbFzD6QNq0zWu5FM/vBUBafqxbPKqdqUpFOBSciIiIySuX3Y94b3f1ID9PuB7a6+51mVglsN7NV7t4GtLj7kkFXOoYkEkbtwmr+47m9tLR1MqEwL+6SRERERCTNUHW3cKDMzAyYCBwDOoZo3WNSXU01re1Jnnq1Ie5SRERERCRDtiHZgbVm9pyZ3dvN9C8CC4ADwGbgY+6ejKYVm9kGM3vazO7q6Q7M7N5ovg0NDWM/OC6/aAqTJxSoy4WIiIjIKJRtSL7O3a8G3gbcb2bXZ0yvA14EZgBLgC+a2aRo2lx3Xwq8B/i8mV3c3R24+0PuvtTdl1ZWVvb7geSagrwEKy6fxrpth+noTPa9gIiIiIiMmKxCsrvvj64PA48CyzJm+QDwXQ92AK8Dl2csuxN4ErhqSCofA2prqjne0s4zrx+LuxQRERERSdNnSDazUjMrSw0DtcBLGbPtAVZE81QBlwE7zazCzIqi8VOBNwNbh6783HbD/EqKCxLqciEiIiIyymTTklwF/NzMNgLPAKvd/TEzu8/M7ovm+QzwJjPbDKwD/jg6E8YCYEO07E+Bz7m7QnJkQmEeb7m0krVb63H3uMsRERERkUifp4CLukks7mb8g2nDBwgtzJnz/Ddw5SBrHNPqaqp5fGs9m/YdZ/Hs8rjLERERERH0j3uxu3nBNPISpi4XIiIiIqOIQnLMyksKWX7hFIVkERERkVFEIXkUqKup5rWGU+w43Bx3KSIiIiKCQvKoUFtTBcDarWpNFhERERkNFJJHgemTJ7B41mTWbKmPuxQRERERQSF51KitqWbj3iYOHW+NuxQRERGRcU8heZSoU5cLERERkVFDIXmUuGRaGRdVluosFyIiIiKjgELyKFJXU83TO4/RdLot7lJERERExjWF5FGkrqaazqSzbtvhuEsRERERGdcUksc4OmYAABpFSURBVEeRRTMnUz2pWF0uRERERGKmkDyKJBJGbU0VT73aQEtbZ9zliIiIiIxbCsmjTF1NNa3tSZ56tSHuUkRERETGLYXkUWbZhVOYPKFAXS5EREREYqSQPMoU5CVYcfk01m07THtnMu5yRERERMYlheRRqLammuMt7Tzz+rG4SxEREREZlxSSR6Eb5ldSXJBQlwsRERGRmCgkj0ITCvO4/tJK1m6pJ5n0uMsRERERGXcUkkepuppqDp1oZdP+43GXIiIiIjLuKCSPUisWTCMvYaxVlwsRERGREaeQPEqVlxRy7UVT1C9ZREREJAYKyaNYXU01rzWcYsfh5rhLERERERlXFJJHsVsWVgGoNVlERERkhCkkj2LTJ09g8azJ6pcsIiIiMsKyCslmtsvMNpvZi2a2oZvpk83sh2a20cy2mNkH0qbdY2avRpd7hrL48aC2ppqN+45z8HhL3KWIiIiIjBv9aUm+0d2XuPvSbqbdD2x198XAW4G/M7NCM5sC/DmwHFgG/LmZVQy26PGkrqYagLVb6mOuRERERGT8GKruFg6UmZkBE4FjQAdQBzzu7sfcvRF4HLh1iO5zXLhk2kQurixVv2QRERGREZRtSHZgrZk9Z2b3djP9i8AC4ACwGfiYuyeBmcDetPn2RePOY2b3mtkGM9vQ0NCQ9QMYD+pqqln/+jGaTrfFXYqIiIjIuJBtSL7O3a8G3gbcb2bXZ0yvA14EZgBLgC+a2aT+FOLuD7n7UndfWllZ2Z9Fx7y6mmo6k866bYfjLkVERERkXMgqJLv7/uj6MPAooX9xug8A3/VgB/A6cDmwH5idNt+saJz0w6JZk6meVKwuFyIiIiIjpM+QbGalZlaWGgZqgZcyZtsDrIjmqQIuA3YCa4BaM6uIDtirjcZJP5gZtTVVPPVqAy1tnXGXIyIiIjLmZdOSXAX83Mw2As8Aq939MTO7z8zui+b5DPAmM9sMrAP+2N2PuPuxaNqz0eXT0Tjpp7qaalrbk/zsFfXXFhERERlu+X3N4O47gcXdjH8wbfgAoZW4u+W/AnxlEDUKsOzCKUyeUMDaLYe49YrquMsRERERGdP0j3s5oiAvwYoF0/jJtnraO5NxlyMiIiIypikk55C6mmpOtHawfqd6rIiIiIgMJ4XkHHL9pZUUFyRYu1VnuRAREREZTgrJOWRCYR43zK9k7ZZ6kkmPuxwRERGRMUshOcfU1VRz6EQrm/Yfj7sUERERkTFLITnH3HT5NPISpj8WERERERlGCsk5prykkGsvmqKQLCIiIjKMFJJzUF1NNTsbTrHj8Mm4SxEREREZkxSSc1DtwvBnImu21MdciYiIiMjYpJCcg6onF7N4drm6XIiIiIgME4XkHFVXU8Wmfcc50NQSdykiIiIiY45Cco6qqwldLh7fqi4XIiIiIkNNITlHXVw5kUumTVSXCxEREZFhoJCcw2oXVrH+9WM0nmqLuxQRERGRMUUhOYfV1VTTmXTWvXw47lJERERExhSF5By2aNZkpk8uVpcLERERkSGmkJzDzIzahVU89UoDp9s64i5HREREZMxQSM5xdTXVnOlI8tQrDXGXIiIiIjJmKCTnuGUXTqG8pIC1+vc9ERERkSGjkJzj8vMSrLi8ip9sq6e9Mxl3OSIiIiJjgkLyGFBXU8WJ1g7W7zwWdykiIiIiY4JC8hjwlksrKS5I6CwXIiIiIkNEIXkMmFCYxw3zK1m79RDJpMddjoiIiEjOU0geI+pqqqk/cYaN+5riLkVEREQk5+VnM5OZ7QJOAp1Ah7svzZj+h8DKtHUuACrd/Vhfy8rQWHF5FfkJY82Weq6aUxF3OSIiIiI5rT8tyTe6+5LuQq67/+9o2hLg48DP3P1YNsvK0JhcUsC1F13A2i2HcFeXCxEREZHBGI7uFncD3xyG9Uof6mqq2HnkFK81NMddioiIiEhOyzYkO7DWzJ4zs3t7msnMSoBbge8MYNl7zWyDmW1oaNC/xw3ELQurAVijPxYRERERGZRsQ/J17n418DbgfjO7vof57gR+kdHVIqtl3f0hd1/q7ksrKyuzrV/SVE8uZsnscp0KTkRERGSQsgrJ7r4/uj4MPAos62HWd5PR1aIfy8oQqK2pYtO+4xxoaom7FBEREZGc1WdINrNSMytLDQO1wEvdzDcZuAH4fn+XlaFTVxO6XKxVa7KIiIjIgGXTklwF/NzMNgLPAKvd/TEzu8/M7kub71eAte5+qq9lh6p4Od/FlRO5ZNpE9UsWERERGYQ+z5Ps7juBxd2MfzDj9leBr2azrAyvupoqHvzZThpPtVFRWhh3OSIiIiI5R/+4NwbV1VTTmXR+sk2tySIiIiIDoZA8Bl05czIzJhezdqtCsoiIiMhAKCSPQWZGbU01T73SwOm2jrjLEREREck5CsljVG1NFWc6kjz1iv6YRURERKS/FJLHqGXzplBeUqCzXIiIiIgMgELyGJWfl2DF5VWs21ZPe2cy7nJEREREcopC8hhWV1PFidYOnt55NO5SRERERHKKQvIYdv38SiYU5LFG/74nIiIi0i99/pmI5K7igjxumF/J2i31fPrtV5BIWNwliYiIyDiWTDqHT55hb+Np9jWeZu+xFvYeO01tTTW3LKyKu7wuFJLHuLorqnhsyyE27mviqjkVcZcjIiIiY5i7c/RUG/saQ/gNYTgM72tsYX9jC20Zx0pNKyuiZsakmCrumULyGHfTZVXkJ4w1W+oVkkVERGTQjre0nw29oTU4CsJRID7d1tll/oqSAmZPKWHh9EnULqxi1pQSZldMYPaUEmaWT6C4IC+mR9I7heQxbnJJAddedAFrthzi926ZT2G+uqGLiIhIz063dXRp/U21CO89FkLxidauf1Q2sSifWRUTmHtBKdddUsnsKROYVVFy9npiUW7GzdysWvrlHUtm8Iff3sSbPvcEv37NLN59zRxmTymJuywRERGJwZmOTg40tZ7XHWJvYwv7jp3m6Km2LvMXFySYVVHCrIoJvGFuBbOnTGB2RcnZIDx5QgFmY++4J3P3uGs4z9KlS33Dhg1xlzFmuDtPbm9g1frdPPHyYRx46/xKVi6fy42XTyNPB/SJiIiMGR2dSQ4ebz3XBSIVgKPW4PqTraTHv4I8Y0Z5CL6p1t9ZUXeIWRUTqJxYNCZDMICZPefuS7udppA8vuxvauHfn9nDt57dy+GTZ5gxuZh3L5vDr18zm6pJxXGXJyIiIn1IJp2G5jPnWoKPtZzrDtF0mgNNrXQmz+W7hMH0yROYWdE1CKf6BVdNKh63DWYKyXKe9s4k67bVs2r9Hv7r1SPkJYzahVWsXD6XN118gU4XNwqcbG3nJ9vqOdDUyqTifMqKCyjrch0NF+Vre4mIjEHJpLOjoZnndzfy0oHj7DkWukPsa2qhraPrGSIqy4qYXXGuL3B6d4jpkyfomKQeKCRLr3YdOcU3n9nDIxv20ni6nXkXlPCe5XP41TfMZkppYdzljSvNZzpYt62eH206yM9eaThvJ9iTiUX5XYNzcT6TMkK1graIyOh2vKWdF/c28fzuRp7f08iLe5s4GR0kV1acz7wLSru0AoezRIQuEaP1DBGjnUKyZKW1vZPHXjrEqvW7eXZXI4V5CW67spqV185l6dyKMdsfKW6n2zpYt+0wqzcd5KfbD3OmI0n1pGJuu3I6ty+azsLpkzh5pp2TrR3Rpb3L9YluxnWd3k57Z9/v81TQnpQZoBW0RUSGXDLpvNbQzPN7Gnl+dxPP72lkR0Mz7qF7xPyqMq6eW8HVcyq4ek45F04t1efwMFBIln7bfugkD6/fzXef38/JMx3Mr5rIyuVz+ZWrZzKpuCDu8nJeS1snP90egvG6l+tpbU9SWVbE7VEwfsOciiELnO7OmY4kJ7oJ0NkG7ZOtHeed/D2TGUwszAzXfQftiUX54VKcT2lRHkX5ag0RkbHnRGs7L+4JYfj5PU28uKfx7KnUyksKuGp2eQjEcytYPLs8Z0+blmsUkmXATrd18MONB1i1fg+b9h1nQkEeb188g5XXzmHRrPK4y8spre2dPLm9gdWbD7JuWz2n2zqZOrGQt10RgvE186aM6gMnWts7hz1oQzjKemJRPqWp8JwaLs5nYmEqTOczsSiPiUUFlBblUVacT2lhGF9WfG7ZovyEWl5EZMQlk87OI81nW4if39PIq4dDK7EZXFZVxlVRC/HVcyu4SK3EsVFIliGxaV8TD6/fw/dfPEBLeydXzpzMe6+dw52LZ1BSqG+83TnT0clTrxxh9aYDPL61nlNtnUwpLeTWK6q548rpLL/oglEdjIdaa3vn2S4gqQB96kwHzWc6o+vo0trR5fapMx2cjK5Pnemk+UxH33cG5CcsI2znMbG4IArY5wfxnsL2xKJ8igsUuEWkeydbU32JQyh+Ia2VePKEAq6aE7USz6lg8ezJlOkX2VFDIVmG1InWdr73wn6+8fRuXqlvpqwon3dePZP3LJ/LZdVlcZcXu7aOJD/f0cCPNh3k8S31nDzTQXlJAbfWVHP7oum88aILyM/TUcaDkUw6p9s7aW49F6K7BOy2rmE7PWCnhs8G8bYOstkN5iWM0sK8tK4hGS3dma3eRXnkJxIkzEgYJBJ2btgMi64TZiQSacMGljZfXiJj3mh6XqKHdWXcV/q6erovEcleaCU+dTYMP7+7iVcOnzzbSjx/WhlXzy2PWopDK7GO1xi9FJJlWLg7z+1uZNX6PazedJC2ziTXzKtg5fK53HpF9bg60ra9M8kvdhxh9aaDrNlyiBOtHUwqzqcuCsZvvmQqBQrGo1Iy6bS0d3YJ02fDd1tquJPmM+1nW7FTQfxkayp8n1s2Ofp2qX1KGFEY7xqoLS2oZwbu4oI8KkoKqCgppKK0kIqSAspLCpmSMVwezaPXv+Sqk63tbNx7/Gy3iRf2NHG8pR2AScX5Z8Pw1XPLWTy7XMft5BiFZBl2x0618e3n9vLw+j3sOnqaipIC3rV0Nncvm8OFU0vjLm9YdHQm+eXOo6zedJDHthyi6XQ7ZUX53FJTxR2LpnPdJZU6L+U44x4Cd3PUat2ZTJJ06Ew6SXfcIelOMnWdTBvOnB4tk5ruacOdyfPX5e50Js+fN+lE07pfV9caiKb52bq7W1fSnVNtnTSdbqPxVDtNp9s4drqN1vae+5yXFeVTXhqF6pIQpEO47jpcXlIQBe3CcfVFW0YHd+e1hlOhhXhPEy/saWR7/blW4kunTTzbbeLqueVcNHWiWolznEKyjJhk0vnv146yav1u1m6tpzPpXHfJVFYun8PNC6tyvjWpM+ms33mUH20+yGMvHeLYqTZKC/O4ZWEVty+awVsunaoPdhm3Wts7aTzdxrFTbTSdbqfxdBuNp9pozBhOheqmU+2c7KV/eXFBgiklhZSXFFKRHrCj1uruhksL88Z1F5LU2WzaOpO0daRdOs9dF+UnKCsq0Bll6L2VuOxsK3HoT7xkjlqJx6JBh2Qz2wWcBDqBjsyVmdkfAiujm/nAAqDS3Y+Z2a3AF4A84Mvu/rm+7k8heWyoP9HKI8/u5ZvP7OHA8VYqy4p49zWzefeyOcwsnxB3eVnrTDrP7jrG6k0H+c+XDnKkuY2SwjxWLKji9iun89bLKhWMRQaorSNJU0sI1SFchyCdPhzCdTTP6TaOt7T32I+8IM9CV4+Sc109UkE6dP84vxV7UnHBgFsDk0mnrTMZgmm34bTz7LQz3YXWaPhMxjLdzd/b8mH+zqzOiZ6pMC8R9aOPDlo926/+3EGs6adqTB8uS+uHX1qYP6oPRHaP+hLv7ruV+Ko55VxcqVbi8WCoQvJSdz+Sxbx3Ar/r7jeZWR7wCnALsA94Frjb3bf2tg6F5LGlM+k8uf0wq9bv4afbD2PATZdPY+XyuVw/v3JU7lSTSee5PY2s3nSQH28+yOGTZyguSLDi8ipuXzSdGy+bxoRCBWOROHQmnRMtITCnunycHU4L1eeGQ+t1Rw8dxhMG5amuHiWFTCzOp6PTQ/A8G0Y7zwvAZzqSPa5zIArzEhTmJyjKD9eF+Ymz49KHi9JuF+XnnZvewzzp0wryE7R1JM/2u28+c65vfWo4vf996nZvXWnSlRbmnQ3N54Xts7ejM8ycHe4awMuKh+b0jc1nOtiY9u91L+xtoul0963Ei2eXM3mCWonHo95C8nCct+tu4JvR8DJgh7vvjAr5FvAOoNeQLGNLXsJYsaCKFQuq2Nd4mm89s5dvPbuXn2x7lpnlE3jP8jm8a+ksppUVx1qnu/P8nqazwfjQiVaK8hPceNk0bl80nZsun0apTu4uEru8hIWW4NLCrJdxd06e6aDpVOj6kR6wQ6A+F6qPnWojP2EU5ieYXFgQhdEeQmiXwJp+O6/rtILu5uk6PJq7iXR0JqOzw7SfOy1j67mzyKSfXebscHT7aPPpEMSjg107s/hikX76xrLirmeOKSvqppU7uj7a3MZzexp5fncjr9SfPHsg7aXTJlK3sJqr54ZQrFZiyUa2LcmvA42AA19y94d6mK+E0GJ8SdTV4leBW939g9H03wCWu/uHu1n2XuBegDlz5rxh9+7dA3xIkgvaOpL8ZFs9q9bv5hc7jpKfMOpqqlm5fA5vvPiCEfuwcHc27jvO6k0H+PHmQ+xvaqEwL8ENl1Vyx6LprFhQpX89EhEZIqk+0+e3Xp87S0xzWmt2qiW7OTqfenNre9oZZjq7vY+yonyWzDn373VL1EosvRiKluTr3H2/mU0DHjezl939qW7muxP4hbsf62+RUfB+CEJ3i/4uL7mlMD/BbVdO57Yrp7OzoZmH1+/h28/vY/Xmg1w0tZT3LJ/Dr75hFuUl2bcUZcvdeWn/CX60+QCrNx1kX2MLBXnG9ZdW8vu187l5YZUOzhARGQYWnT6wuCCPyrKiQa0rmfQu50RvPtPBxKJ8tRLLkOn32S3M7AGg2d3/tptpjwL/4e4PR7ffCDzg7nXR7Y8DuPtf9XYf6pM8PrW2d/LjzQdZtX4Pz+1upDA/wR2LprNy+VyunlM+qNZld2frwROs3nSQ1ZsPsvvoafITxnWXTuX2K6dTu7CaySUKxiIiIuPJoA7cM7NSIOHuJ6Phx4FPu/tjGfNNBl4HZrv7qWhcPuHAvRXAfsKBe+9x9y293adCsmw7eIKH1+/h0Rf203ymg8ury1h57VzuWjIj67/zdHe2158MwXjTQXYeOUVewnjTxRdwx6IQjPvTp1FERETGlsGG5IuAR6Ob+cDD7v6XZnYfgLs/GM33fkL/43dnLH8b8HnCKeC+4u5/2VfBCsmScupMBz/YeIBvPL2bLQdOUFKYxzuWzGTl8jlcMXNyt8vsOHySH24MLcY7DjeTMLj2ogu4Y9EM6mqquGDi4H7iExERkbFBfyYiOc/d2bTvOKvW7+YHGw/Q2p5k8exyVi6fw52LZnDgeMvZFuPt9Scxg2XzpnDH4hncWlM96L5vIiIiMvYoJMuYcrylnUef38eq9Xt49XAzRfkJznSEc3heM6+COxbN4G1XVDNtUrynlBMREZHRbaTPkywyrCZPKOD9b76Qe940j2d3NfLDjQe4cGopt105nerJCsYiIiIyeArJkrPMjGUXTmHZhVPiLkVERETGmETcBYiIiIiIjDYKySIiIiIiGRSSRUREREQyKCSLiIiIiGRQSBYRERERyaCQLCIiIiKSQSFZRERERCSDQrKIiIiISIZR+bfUZtYA7I67jjFmKnAk7iJkQLTtcpe2XW7Sdstd2na5K65tN9fdK7ubMCpDsgw9M9vQ03+Ty+imbZe7tO1yk7Zb7tK2y12jcdupu4WIiIiISAaFZBERERGRDArJ48dDcRcgA6Ztl7u07XKTtlvu0rbLXaNu26lPsoiIiIhIBrUki4iIiIhkUEgWEREREcmgkDzGmdlsM/upmW01sy1m9rG4a5LsmVmemb1gZj+KuxbJnpmVm9m3zexlM9tmZm+MuybJjpn9brSvfMnMvmlmxXHXJN0zs6+Y2WEzeylt3BQze9zMXo2uK+KsUc7Xw3b739H+cpOZPWpm5XHWmKKQPPZ1AL/v7guBa4H7zWxhzDVJ9j4GbIu7COm3LwCPufvlwGK0DXOCmc0EPgosdfcrgDzg3fFWJb34KnBrxrg/Ada5+6XAuui2jC5f5fzt9jhwhbsvAl4BPj7SRXVHIXmMc/eD7v58NHyS8GE9M96qJBtmNgu4Hfhy3LVI9sxsMnA98C8A7t7m7k3xViX9kA9MMLN8oAQ4EHM90gN3fwo4ljH6HcDXouGvAXeNaFHSp+62m7uvdfeO6ObTwKwRL6wbCsnjiJnNA64C1sdbiWTp88AfAcm4C5F+uRBoAP416irzZTMrjbso6Zu77wf+FtgDHASOu/vaeKuSfqpy94PR8CGgKs5iZEB+E/jPuIsAheRxw8wmAt8BfsfdT8Rdj/TOzO4ADrv7c3HXIv2WD1wN/F93vwo4hX7yzQlR/9V3EL7ozABKzey98VYlA+XhHLc6z20OMbM/JXQTXRV3LaCQPC6YWQEhIK9y9+/GXY9k5c3A281sF/At4CYz+0a8JUmW9gH73D31i823CaFZRr+bgdfdvcHd24HvAm+KuSbpn3ozmw4QXR+OuR7Jkpm9H7gDWOmj5E88FJLHODMzQt/Ibe7+93HXI9lx94+7+yx3n0c4cOgJd1eLVg5w90PAXjO7LBq1AtgaY0mSvT3AtWZWEu07V6CDLnPND4B7ouF7gO/HWItkycxuJXQvfLu7n467nhSF5LHvzcBvEFoiX4wut8VdlMgY9xFglZltApYAn425HslC1Pr/beB5YDPhM3LU/VWuBGb2TeCXwGVmts/M/gfwOeAWM3uV8MvA5+KsUc7Xw3b7IlAGPB7llAdjLTKiv6UWEREREcmglmQRERERkQwKySIiIiIiGRSSRUREREQyKCSLiIiIiGRQSBYRERERyaCQLCIiIiKSQSFZRERERCTD/w+0elrZUEP4ywAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
        "'''\n",
        "if USE_CUDA:\n",
        "  best_model = best_model.cuda()\n",
        "val_loss = evaluate(best_model, val_iter)\n",
        "print(\"perplexity: \", np.exp(val_loss))"
      ],
      "metadata": {
        "id": "lzfY3BA-8JF0",
        "outputId": "34d8de35-edef-4c1b-bfdd-f4019b20e73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  271.2047316660911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
        "'''\n",
        "test_loss = evaluate(best_model, test_iter)\n",
        "print(\"perplexity: \", np.exp(test_loss))"
      ],
      "metadata": {
        "id": "-61pscNZ8JIH",
        "outputId": "644d9c58-b10a-4185-bfc3-2b081b059f96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  257.0131939681911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence generation in Hindi\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# predict next token\n",
        "def predict(best_model, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[TEXT.vocab.stoi[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = repackage_hidden(h)\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = best_model(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out.view(-1, out.size(-1)) , dim=1)\n",
        "\n",
        "  # get indices of top 3 or 5 values\n",
        "  top_n_idx = p.argsort().reshape(-1)[-3:]\n",
        "  #top_n_idx = p.argsort()[-1][-3:]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  #print(sampled_token_index)\n",
        "\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return TEXT.vocab.itos[sampled_token_index], h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(best_model, size, prime='it is'):\n",
        "        \n",
        "    # push to GPU\n",
        "    #best_model.cuda()\n",
        "    \n",
        "    best_model.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = None\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(best_model, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(best_model, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)\n",
        "\n",
        "words_list = [\"सिनेमा\", \"प्राचीन\", \"मलयालम\", \"शरीर\", \"भौतिक\", \"प्रयोग\", \"एक\"]\n",
        "for i in words_list:\n",
        "  print(i,\" :\", sample(best_model, 50, prime=i))"
      ],
      "metadata": {
        "id": "bBNti1wu8JKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561510cc-23cc-45e3-eac8-996e51fb81b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "सिनेमा  : सिनेमा का सामना करना पड़ा. इस प्रकार के <unk> के लिए एक बार जब ब्लोर के <unk> के लिए एक नया <unk> था जिसे <unk> के लिए किया जाता था, जिसे <unk> कहा गया। इस तरह के लिए एक <unk> के लिए एक विशेष रूप से, एक <unk> <unk> का एक\n",
            "प्राचीन  : प्राचीन <unk> प्रस्तर साहित्य में स्थित है । यह <unk> का एक प्रमुख अंग है, जो कि <unk> का एक महत्वपूर्ण अंग है। इस ग्रंथ में एक प्रमुख परकोटा <unk> <unk> <unk> <unk> और <unk> <unk> का <unk> <unk> <unk> <unk> और <unk> <unk> का <unk> <unk> के साथ ही साथ\n",
            "मलयालम  : मलयालम के रूप से लेकर <unk> के <unk> <unk> <unk> के साथ ही साथ <unk> और अन्य अन्य संस्कृतियां उल्टी <unk> और वृत्ताकार, लाइअर के <unk> के साथ <unk> के साथ साध्यता के <unk> <unk> और <unk> <unk> जैसे बांसुरी, क्र, <unk> <unk> <unk> <unk> <unk> आदि। <eos> <eos> <unk> <unk>\n",
            "शरीर  : शरीर के कारण ही न ही एक ही <unk> या किसी एक तरह के लिए एक दूसरे के लिए <unk> होता है। <eos> <unk> के कैटरपिलर के लिए एक दूसरे को एक ही समय में एक दूसरे से जुड़ते हैं। <eos> कैटरपिलर का प्रयोग एक <unk> के साथ एक <unk> के\n",
            "भौतिक  : भौतिक <unk> <unk> और <unk> की एक <unk> <unk> के लिए एक <unk> के रूप में <unk> के लिए जाना गया। <eos> <unk> के अनुसार, <unk> के साथ ही साथ <unk> <unk> और झांझ <unk> का एक महत्वपूर्ण अंग है। यह एक ऐसा अनूठा <unk> है जो धातुरूप से <unk> और\n",
            "प्रयोग  : प्रयोग और खाने-पीने की खोज की। यह एक राष्ट्रप्रेमी का एक समूह है, जो <unk> <unk> और उसके साथ <unk> और सुव्यवस्था के बीच <unk> के <unk> के साथ <unk> और वैरागी मनोरोगी शामिल होता है। यह एक लालची वाद्ययंत्र है जिसे एक <unk> के लिए किया जाता है। <unk> <unk>\n",
            "एक  : एक ऐसा सा प्रभाव होता है जो एक दूसरे से जुड़े हुए होते हैं जो सुमेरियन के लिए भी जाना जाता है जो कि एक <unk> के साथ <unk> और <unk> की <unk> है। यह भी <unk> के <unk> में <unk> की ओर ले जाती है जिसे <unk> कहते हैं, लेकिन\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen1 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार एक\"\n",
        "        \"छोटे कुत्ते के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen2 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"समुद्र के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen3 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"एक पेंसिल के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen4 = (\"डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार क्यूबा बराबर होता है।\"\n",
        "         \"प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen5 = sen1.split()\n",
        "random.shuffle(sen5)\n",
        "sen5 = \" \".join(sen5)\n",
        "\n",
        "sen6 = \" \".join(['डैविल एक मांसाहारी धानीप्राणी है']*8)\n",
        "\n",
        "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\n",
        "\n",
        "for sen in sen_list:\n",
        "\n",
        "    print(sen)\n",
        "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\n",
        "        print(sen, file = text_file)\n",
        "\n",
        "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', text_field=TEXT)\n",
        "\n",
        "\n",
        "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \n",
        "                                                  bptt_len=BPTT_LENGTH, repeat=False)\n",
        "        \n",
        "    sen_loss = evaluate(best_model, sen_iter)\n",
        "    print(\"perplexity: \", np.exp(sen_loss))\n",
        "    print()"
      ],
      "metadata": {
        "id": "FpxKIiks8JNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafbbaac-8f16-4784-c846-577c9244b208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार एकछोटे कुत्ते के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  36676.64020487135\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारसमुद्र के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  38459.00898318032\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारएक पेंसिल के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  15501.205243912675\n",
            "\n",
            "डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार क्यूबा बराबर होता है।प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  16448.376581905235\n",
            "\n",
            "तस्मानियाई केवल के है बराबर धानीप्राणी जाता विलुप्त का कुत्ते बन डैविल पाया अब धानीप्राणी एकछोटे के के सबसे एक बड़ा जो आकार ही मांसाहारी के जंगलों मांसाहारी में होने होता में थायलेसीन​ के बाद ऑस्ट्रेलिया दुनिया गया। यह है।1936 तस्मानिया द्वीप राज्य है।इसका\n",
            "perplexity:  113871.19490050692\n",
            "\n",
            "डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है\n",
            "perplexity:  321860.749237856\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(i) Substitute LSTM with a GRU\n",
        "\n",
        "model_GRU = RNNLM(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "print(\"model_GRU:\", model_GRU)\n",
        "print(\"use_cuda_status: \", USE_CUDA)\n",
        "if USE_CUDA:\n",
        "  model_GRU = model_GRU.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model_GRU.parameters(), lr=learning_rate)\n",
        "val_losses = []\n",
        "best_model_GRU = RNNLM(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model_GRU.train()\n",
        "    log_interval = 1000\n",
        "    it = iter(train_iter)\n",
        "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "    hidden = None\n",
        "\n",
        "    for i, batch in enumerate(it):\n",
        "      model_GRU.zero_grad()\n",
        "      text, target = batch.text, batch.target\n",
        "      if USE_CUDA:\n",
        "        text, target = text.cuda(), target.cuda()\n",
        "      output_n, hidden_n = model_GRU(text, hidden)\n",
        "      hidden = repackage_hidden(hidden_n)\n",
        "      loss = loss_fn(output_n.view(-1, output_n.size(-1)), target.view(-1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      nn.utils.clip_grad_norm_(model_GRU.parameters(), max_norm=GRAD_CLIP, norm_type=2.0, error_if_nonfinite=False)\n",
        "\n",
        "      if i % log_interval == 0 and i > 0:\n",
        "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
        "\n",
        "      if i % 2000 == 0 and i > 0:\n",
        "        val_loss = evaluate(model_GRU, val_iter)\n",
        "        print(f'Iteration {i}, the validation loss is {val_loss:.3f}.')\n",
        "        val_losses.append(val_loss)\n",
        "        if val_loss <= min(val_losses):\n",
        "          torch.save(model_GRU.state_dict(), \"gdrive/My Drive/AML_Project/best_model_GRU.pt\")\n",
        "\n",
        "best_model_GRU.load_state_dict(torch.load(\"gdrive/My Drive/AML_Project/best_model_GRU.pt\")) "
      ],
      "metadata": {
        "id": "wYAp3Hez8JPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7724055b-2170-4dfd-ad46-cf44ad87552d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_GRU: RNNLM(\n",
            "  (embedding): Embedding(100002, 300)\n",
            "  (rnn): GRU(300, 300, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=300, out_features=100002, bias=True)\n",
            ")\n",
            "use_cuda_status:  True\n",
            "At iteration 1000 the loss is 7.962.\n",
            "At iteration 2000 the loss is 7.841.\n",
            "Iteration 2000, the validation loss is 7.796.\n",
            "At iteration 3000 the loss is 6.665.\n",
            "At iteration 4000 the loss is 6.094.\n",
            "Iteration 4000, the validation loss is 6.182.\n",
            "At iteration 5000 the loss is 6.059.\n",
            "At iteration 1000 the loss is 5.758.\n",
            "At iteration 2000 the loss is 5.692.\n",
            "Iteration 2000, the validation loss is 5.723.\n",
            "At iteration 3000 the loss is 5.535.\n",
            "At iteration 4000 the loss is 5.112.\n",
            "Iteration 4000, the validation loss is 5.566.\n",
            "At iteration 5000 the loss is 5.346.\n",
            "At iteration 1000 the loss is 5.055.\n",
            "At iteration 2000 the loss is 5.166.\n",
            "Iteration 2000, the validation loss is 5.473.\n",
            "At iteration 3000 the loss is 5.195.\n",
            "At iteration 4000 the loss is 4.718.\n",
            "Iteration 4000, the validation loss is 5.406.\n",
            "At iteration 5000 the loss is 5.039.\n",
            "At iteration 1000 the loss is 4.722.\n",
            "At iteration 2000 the loss is 4.893.\n",
            "Iteration 2000, the validation loss is 5.351.\n",
            "At iteration 3000 the loss is 4.932.\n",
            "At iteration 4000 the loss is 4.453.\n",
            "Iteration 4000, the validation loss is 5.304.\n",
            "At iteration 5000 the loss is 4.881.\n",
            "At iteration 1000 the loss is 4.501.\n",
            "At iteration 2000 the loss is 4.642.\n",
            "Iteration 2000, the validation loss is 5.293.\n",
            "At iteration 3000 the loss is 4.778.\n",
            "At iteration 4000 the loss is 4.240.\n",
            "Iteration 4000, the validation loss is 5.264.\n",
            "At iteration 5000 the loss is 4.768.\n",
            "At iteration 1000 the loss is 4.342.\n",
            "At iteration 2000 the loss is 4.489.\n",
            "Iteration 2000, the validation loss is 5.269.\n",
            "At iteration 3000 the loss is 4.684.\n",
            "At iteration 4000 the loss is 4.176.\n",
            "Iteration 4000, the validation loss is 5.234.\n",
            "At iteration 5000 the loss is 4.617.\n",
            "At iteration 1000 the loss is 4.261.\n",
            "At iteration 2000 the loss is 4.415.\n",
            "Iteration 2000, the validation loss is 5.255.\n",
            "At iteration 3000 the loss is 4.562.\n",
            "At iteration 4000 the loss is 4.009.\n",
            "Iteration 4000, the validation loss is 5.229.\n",
            "At iteration 5000 the loss is 4.597.\n",
            "At iteration 1000 the loss is 4.148.\n",
            "At iteration 2000 the loss is 4.340.\n",
            "Iteration 2000, the validation loss is 5.243.\n",
            "At iteration 3000 the loss is 4.550.\n",
            "At iteration 4000 the loss is 3.999.\n",
            "Iteration 4000, the validation loss is 5.220.\n",
            "At iteration 5000 the loss is 4.503.\n",
            "At iteration 1000 the loss is 4.101.\n",
            "At iteration 2000 the loss is 4.327.\n",
            "Iteration 2000, the validation loss is 5.241.\n",
            "At iteration 3000 the loss is 4.436.\n",
            "At iteration 4000 the loss is 3.873.\n",
            "Iteration 4000, the validation loss is 5.212.\n",
            "At iteration 5000 the loss is 4.472.\n",
            "At iteration 1000 the loss is 4.036.\n",
            "At iteration 2000 the loss is 4.219.\n",
            "Iteration 2000, the validation loss is 5.240.\n",
            "At iteration 3000 the loss is 4.387.\n",
            "At iteration 4000 the loss is 3.809.\n",
            "Iteration 4000, the validation loss is 5.224.\n",
            "At iteration 5000 the loss is 4.416.\n",
            "At iteration 1000 the loss is 3.999.\n",
            "At iteration 2000 the loss is 4.184.\n",
            "Iteration 2000, the validation loss is 5.244.\n",
            "At iteration 3000 the loss is 4.378.\n",
            "At iteration 4000 the loss is 3.821.\n",
            "Iteration 4000, the validation loss is 5.216.\n",
            "At iteration 5000 the loss is 4.415.\n",
            "At iteration 1000 the loss is 3.968.\n",
            "At iteration 2000 the loss is 4.138.\n",
            "Iteration 2000, the validation loss is 5.251.\n",
            "At iteration 3000 the loss is 4.316.\n",
            "At iteration 4000 the loss is 3.763.\n",
            "Iteration 4000, the validation loss is 5.216.\n",
            "At iteration 5000 the loss is 4.372.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity score on validation and test datasets\n",
        "'''\n",
        "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
        "'''\n",
        "if USE_CUDA:\n",
        "  best_model_GRU = best_model_GRU.cuda()\n",
        "val_loss = evaluate(best_model_GRU, val_iter)\n",
        "print(\"perplexity on validation set: \", np.exp(val_loss))\n",
        "\n",
        "'''\n",
        "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
        "'''\n",
        "test_loss = evaluate(best_model_GRU, test_iter)\n",
        "print(\"perplexity on test set: \", np.exp(test_loss))"
      ],
      "metadata": {
        "id": "O86dXrtL8JRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982e19f8-d4d0-4fc2-aba2-57a929634d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity on validation set:  183.41251158988106\n",
            "perplexity on test set:  176.19636645956507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence generation in Hindi using GRU model\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# predict next token\n",
        "def predict(best_model_GRU, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[TEXT.vocab.stoi[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = repackage_hidden(h)\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = best_model(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out.view(-1, out.size(-1)) , dim=1)\n",
        "\n",
        "  # get indices of top 3 or 5 values\n",
        "  top_n_idx = p.argsort().reshape(-1)[-3:]\n",
        "  #top_n_idx = p.argsort()[-1][-3:]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  #print(sampled_token_index)\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return TEXT.vocab.itos[sampled_token_index], h\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(model, size, prime='it is'):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = None\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(best_model, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(best_model, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)\n",
        "\n",
        "words_list = [\"सिनेमा\", \"प्राचीन\", \"मलयालम\", \"शरीर\", \"भौतिक\", \"प्रयोग\", \"एक\"]\n",
        "for i in words_list:\n",
        "  print(i,\" :\", sample(best_model_GRU, 50, prime=i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDyXVTz_Fczv",
        "outputId": "5a64dd93-8449-4a39-f54c-0a7d0ed9fda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "सिनेमा  : सिनेमा के लिए एक नया और हिस्सा था। इस प्रकार, एक बार एक नया आश्चर्य है की इस तरह के एक नज़रिये है जो कि एक <unk> के लिए dfa के रूप में, <unk> <unk> के रूप में जाना गया है। <eos> एक अन्य कैटरपिलर के साथ एक <unk> के रूप\n",
            "प्राचीन  : प्राचीन और अरुणाचल में <unk> के रूप में, यह <unk> की तरह के लिए जाना जा रहा है कि <unk> <unk> के <unk> <unk> के रूप में एक नया स्थान था। <unk> में एक रंटजन के <unk> में से एक ही <unk> का एक हिस्सा <unk> के <unk> के लिए <unk>\n",
            "मलयालम  : मलयालम और अधिक <unk> के साथ ही <unk> <unk> <unk> और <unk> के बीच के रूप में। यह <unk> के लिए <unk> <unk> के <unk> में <unk> की <unk> <unk> के साथ ही साथ <unk> और <unk> के बीच के एक <unk> का <unk> <eos> <unk> <eos> <unk> <unk> के रूप\n",
            "शरीर  : शरीर ही <unk> <unk> और अन्य संस्कृतियों के लिए भी एक विशेष <unk> के लिए एक <unk> का प्रयोग किया गया है। यह पराभक्ति का एक महत्वपूर्ण अंग है जो <unk> के साथ <unk> में स्थित है। <eos> ग्रिल्स के अनुसार, अली के पिता ने <unk> के रूप में अपने <unk>\n",
            "भौतिक  : भौतिक ही <unk> है। <eos> <unk> के अनुसार, एक ही समय में, <unk> के लिए <unk> <unk> की एक श्रृंखला का एक उदाहरण है। <eos> आर्यभट ने कहा कि <unk> के लिए एक ही एक <unk> के साथ एक साथ एक ही के रूप में <unk> के साथ काम करना था।\n",
            "प्रयोग  : प्रयोग की और यह <unk> की एक श्रृंखला के लिए एक <unk> है। <unk> <unk> के साथ <unk> में एक बार एक ही <unk> का उपयोग करता है और इस प्रकार की एक शुरुआत, जो की <unk> <unk> <unk> की <unk> के साथ ही <unk> <unk> और एक nsaid में <unk>\n",
            "एक  : एक विशेष महत्व था और इस तरह यह भी पता चला था क्योंकि वह अपने कानून-व्यवस्था के रूप मे एक बार के रूप में। इस समय से एक ही <unk> <unk> और एक <unk> के साथ ही <unk> में एक बार <unk> के रूप में यह <unk> के साथ मेल खाता\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity scoring for the 5 setences using GRU model\n",
        "\n",
        "sen1 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार एक\"\n",
        "        \"छोटे कुत्ते के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen2 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"समुद्र के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen3 = (\"तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार\" \n",
        "        \"एक पेंसिल के बराबर होता है।\"\n",
        "         \"1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen4 = (\"डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।\" \n",
        "        \"इसका आकार क्यूबा बराबर होता है।\"\n",
        "         \"प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\")\n",
        "\n",
        "sen5 = sen1.split()\n",
        "random.shuffle(sen5)\n",
        "sen5 = \" \".join(sen5)\n",
        "\n",
        "sen6 = \" \".join(['डैविल एक मांसाहारी धानीप्राणी है']*8)\n",
        "\n",
        "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\n",
        "\n",
        "for sen in sen_list:\n",
        "\n",
        "    print(sen)\n",
        "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\n",
        "        print(sen, file = text_file)\n",
        "\n",
        "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', text_field=TEXT)\n",
        "\n",
        "\n",
        "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \n",
        "                                                  bptt_len=BPTT_LENGTH, repeat=False)\n",
        "        \n",
        "    sen_loss = evaluate(best_model_GRU, sen_iter)\n",
        "    print(\"perplexity: \", np.exp(sen_loss))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTkAQFIgFc2r",
        "outputId": "395abf31-35b3-4036-daad-143e4c96259b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार एकछोटे कुत्ते के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  599.905645076266\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारसमुद्र के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  417.4144243634351\n",
            "\n",
            "तस्मानियाई डैविल एक मांसाहारी धानीप्राणी है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकारएक पेंसिल के बराबर होता है।1936 में थायलेसीन​ के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  248.66167909837358\n",
            "\n",
            "डैविल एक मांसाहारी प्रधानमंत्री है जो अब केवल ऑस्ट्रेलिया के द्वीप राज्य तस्मानिया के जंगलों में ही पाया जाता है।इसका आकार क्यूबा बराबर होता है।प्लास्टिक के विलुप्त होने के बाद यह दुनिया का सबसे बड़ा मांसाहारी धानीप्राणी बन गया।\n",
            "perplexity:  146.8859519173458\n",
            "\n",
            "होता एक राज्य द्वीप है।1936 जंगलों के सबसे जाता बराबर मांसाहारी जो है।इसका थायलेसीन​ बाद के है के आकार केवल के धानीप्राणी धानीप्राणी में कुत्ते विलुप्त तस्मानिया अब तस्मानियाई बड़ा एकछोटे में मांसाहारी का के ही होने दुनिया बन पाया यह ऑस्ट्रेलिया गया। डैविल\n",
            "perplexity:  1610.6824374150813\n",
            "\n",
            "डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है डैविल एक मांसाहारी धानीप्राणी है\n",
            "perplexity:  1271.125326850815\n",
            "\n"
          ]
        }
      ]
    }
  ]
}